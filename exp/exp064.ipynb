{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e87ca9a-aa97-4bf7-87a1-bc00cc707bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 26 07:58:54 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   35C    P8    20W / 300W |      0MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2053bec0-e260-4298-8646-e19b452a23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    AUTHOR = \"wanwan7123\"\n",
    "\n",
    "    NAME = \"feedback3-Exp064-electra-large\"\n",
    "    MODEL_PATH = \"google/electra-large-discriminator\"\n",
    "    DATASET_PATH = []\n",
    "\n",
    "    COMPETITION = \"feedback-prize-english-learning\"\n",
    "\n",
    "    api_path = \"kaggle_json/kaggle.json\"\n",
    "\n",
    "    apex=True\n",
    "    seed = 42\n",
    "    num_fold = 10\n",
    "    trn_fold = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    batch_size = 16\n",
    "    n_epochs = 5\n",
    "    max_len = 512\n",
    "    target_list = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
    "    \n",
    "    weight_decay = 0.01\n",
    "    scheduler='cosine'\n",
    "    betas = (0.9, 0.999)\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-5\n",
    "    lr_weight_decay = 0.97\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    eval_step = 30\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps_rate=0.1\n",
    "    clip_grad_norm = 1000\n",
    "    gradient_accumulation_steps = 1\n",
    "    \n",
    "    # GPU Optimize Settings\n",
    "    gpu_optimize_config= {\n",
    "        \"freezing\": False,\n",
    "        \"gradient_checkpoint\": True\n",
    "    }\n",
    "\n",
    "    upload_from_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb438435-24a6-4720-96e4-7fa73c0a4065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iterative-stratification\n",
      "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.23.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.10.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp39-cp39-linux_x86_64.whl (1821.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.1+cu113) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0+cu116\n",
      "    Uninstalling torch-1.12.0+cu116:\n",
      "      Successfully uninstalled torch-1.12.0+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu116 requires torch==1.12.0, but you have torch 1.10.1+cu113 which is incompatible.\n",
      "torchaudio 0.12.0+cu116 requires torch==1.12.0, but you have torch 1.10.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.10.1+cu113\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting text-unidecode\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: text-unidecode\n",
      "Successfully installed text-unidecode-1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import joblib\n",
    "import random\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ast import literal_eval\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, \n",
    "    KFold, \n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "! pip install iterative-stratification\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "! pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "!pip install text-unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83900b8-430a-440d-8ac6-d696e4ffb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(cfg):\n",
    "    cfg.COLAB = 'google.colab' in sys.modules\n",
    "    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # pip install\n",
    "    ! pip install transformers==4.16.2\n",
    "    ! pip install tokenizers==0.11.6\n",
    "    ! pip install transformers[sentencepiece]\n",
    "\n",
    "    # use kaggle api (need kaggle token)\n",
    "    f = open(cfg.api_path, 'r')\n",
    "    json_data = json.load(f) \n",
    "    os.environ['KAGGLE_USERNAME'] = json_data['username']\n",
    "    os.environ['KAGGLE_KEY'] = json_data['key']\n",
    "\n",
    "    # set dirs\n",
    "    cfg.INPUT = 'input'\n",
    "    cfg.EXP = cfg.NAME\n",
    "    cfg.OUTPUT_EXP = cfg.NAME\n",
    "    cfg.SUBMISSION = './'\n",
    "    cfg.DATASET = '../input/'\n",
    "\n",
    "    cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n",
    "    cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n",
    "    cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n",
    "\n",
    "    # make dirs\n",
    "    for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name, upload_dir):\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n",
    "    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "    dataset_metadata['title'] = dataset_name\n",
    "    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fcca9f-a99e-4ca0-b230-8ad8e65ca9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Utils\n",
    "# =====================\n",
    "# Seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# KFold\n",
    "def get_kfold(train, n_splits, seed):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train)\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_stratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_groupkfold(train, target_col, group_col, n_splits):\n",
    "    kf = GroupKFold(n_splits=n_splits)\n",
    "    generator = kf.split(train, train[target_col], train[group_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_groupstratifiedkfold(train, target_col, group_col, n_splits, seed):\n",
    "    kf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col], train[group_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_multilabelstratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89beb2d-5c37-43d8-8b8b-1ee1ad722b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcrmse(cfg, preds, df):\n",
    "    all_score = 0\n",
    "    for i, column in enumerate(cfg.target_list):\n",
    "        score = np.sqrt(mean_squared_error(preds[:, i], df[column]))\n",
    "        all_score += score/len(cfg.target_list)\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63201c37-a808-46cf-938d-c412eeef851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章のバグを治す\n",
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3a9b4a-c07c-4b54-8ef2-9264ec0387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Dataset, Model\n",
    "# =====================\n",
    "\n",
    "def processing_features(df):\n",
    "    df['text'] = df['full_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "    return df\n",
    "\n",
    "# dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.text = df['text'].to_numpy()\n",
    "        self.labels = df[cfg.target_list].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.prepare_input(self.cfg, self.text[index])\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        return inputs, label\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_input(cfg, text):\n",
    "        inputs = cfg.tokenizer(text,\n",
    "                               add_special_tokens=True,\n",
    "                               max_length=cfg.max_len,\n",
    "                               padding=\"max_length\",\n",
    "                               truncation=True,\n",
    "                               return_offsets_mapping=False)\n",
    "        inputs['input_ids'] = torch.tensor(\n",
    "            inputs['input_ids'],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        inputs['attention_mask'] = torch.tensor(\n",
    "            inputs['attention_mask'],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6fc2e4e-b7b0-45d6-9068-87c9fbcc2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.gpu_optimize_config = cfg.gpu_optimize_config\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            cfg.MODEL_PATH,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout\": 0.,\n",
    "                \"hidden_dropout_prob\": 0.,\n",
    "                \"attention_dropout\": 0.,\n",
    "                \"attention_probs_dropout_prob\": 0,\n",
    "            }\n",
    "        )\n",
    "        self.backbone = ElectraModel.from_pretrained(\n",
    "            cfg.MODEL_PATH,\n",
    "            config=self.config\n",
    "        )\n",
    "        self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        self.ln = nn.LayerNorm(self.config.hidden_size)\n",
    "        self._init_weights(self.ln)\n",
    "\n",
    "        # Freeze\n",
    "        if self.gpu_optimize_config['freezing']:\n",
    "            freeze(self.backbone.encoder.layer[:8])\n",
    "\n",
    "        # Gradient Checkpointing\n",
    "        if self.gpu_optimize_config['gradient_checkpoint']:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.backbone(**inputs)\n",
    "        last_state = outputs[0]\n",
    "        feature = self.pool(last_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        # batch, hidden_size\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.ln(feature))\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.SmoothL1Loss(reduction='mean')\n",
    "            loss = loss_fct(output, labels)\n",
    "            return loss, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64a1030-4ae5-491f-a5e3-df06d10e9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_optimizer_grouped_parameters(cfg, model):\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "#         no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#         optimizer_parameters = [\n",
    "#             {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#              'lr': cfg.encoder_lr, 'weight_decay': cfg.weight_decay},\n",
    "#             {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#              'lr': cfg.encoder_lr, 'weight_decay': 0.0},\n",
    "#             {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n",
    "#              'lr': cfg.decoder_lr, 'weight_decay': 0.0}\n",
    "#         ]\n",
    "#         return optimizer_parameters\n",
    "    \n",
    "    \n",
    "def get_optimizer_grouped_parameters(cfg, model):\n",
    "    model_type = 'backbone'\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n",
    "             'lr': cfg.decoder_lr, 'weight_decay': 0.0},\n",
    "    ]\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n",
    "    layers.reverse()\n",
    "    lr = cfg.encoder_lr\n",
    "    for layer in layers:\n",
    "        lr *= cfg.lr_weight_decay\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": cfg.weight_decay,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "# initialize layer\n",
    "def reinit_bert(model):\n",
    "    for layer in model.backbone.encoder.layer[-2:]:\n",
    "        for module in layer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "    return model\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=int(num_train_steps*cfg.num_warmup_steps_rate), num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=int(num_train_steps*cfg.num_warmup_steps_rate), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769d9ea0-82a7-4155-9a83-2b6cc3a69411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGM\n",
    "# https://www.kaggle.com/competitions/tweet-sentiment-extraction/discussion/143764#809408\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=0.3, emb_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "726bbd52-23c9-4c89-9b1f-aec02e332c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import InplaceFunction\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.init as init\n",
    "\n",
    "class Mixout(InplaceFunction):\n",
    "    @staticmethod\n",
    "    def _make_noise(input):\n",
    "        return input.new().resize_as_(input)\n",
    "\n",
    "    @classmethod\n",
    "    def forward(cls, ctx, input, target=None, p=0.0, training=False, inplace=False):\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"A mix probability of mixout has to be between 0 and 1,\" \" but got {}\".format(p))\n",
    "        if target is not None and input.size() != target.size():\n",
    "            raise ValueError(\n",
    "                \"A target tensor size must match with a input tensor size {},\"\n",
    "                \" but got {}\".format(input.size(), target.size())\n",
    "            )\n",
    "        ctx.p = p\n",
    "        ctx.training = training\n",
    "\n",
    "        if ctx.p == 0 or not ctx.training:\n",
    "            return input\n",
    "\n",
    "        if target is None:\n",
    "            target = cls._make_noise(input)\n",
    "            target.fill_(0)\n",
    "        target = target.to(input.device)\n",
    "\n",
    "        if inplace:\n",
    "            ctx.mark_dirty(input)\n",
    "            output = input\n",
    "        else:\n",
    "            output = input.clone()\n",
    "\n",
    "        ctx.noise = cls._make_noise(input)\n",
    "        if len(ctx.noise.size()) == 1:\n",
    "            ctx.noise.bernoulli_(1 - ctx.p)\n",
    "        else:\n",
    "            ctx.noise[0].bernoulli_(1 - ctx.p)\n",
    "            ctx.noise = ctx.noise[0].repeat(input.size()[0], 1)\n",
    "        ctx.noise.expand_as(input)\n",
    "\n",
    "        if ctx.p == 1:\n",
    "            output = target\n",
    "        else:\n",
    "            output = ((1 - ctx.noise) * target + ctx.noise * output - ctx.p * target) / (1 - ctx.p)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if ctx.p > 0 and ctx.training:\n",
    "            return grad_output * ctx.noise, None, None, None, None\n",
    "        else:\n",
    "            return grad_output, None, None, None, None\n",
    "\n",
    "\n",
    "def mixout(input, target=None, p=0.0, training=False, inplace=False):\n",
    "    return Mixout.apply(input, target, p, training, inplace)\n",
    "\n",
    "\n",
    "class MixLinear(torch.nn.Module):\n",
    "    __constants__ = [\"bias\", \"in_features\", \"out_features\"]\n",
    "    def __init__(self, in_features, out_features, bias=True, target=None, p=0.0):\n",
    "        super(MixLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "        self.target = target\n",
    "        self.p = p\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, mixout(self.weight, self.target, self.p, self.training), self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        type = \"drop\" if self.target is None else \"mix\"\n",
    "        return \"{}={}, in_features={}, out_features={}, bias={}\".format(\n",
    "            type + \"out\", self.p, self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "def replace_mixout(model):\n",
    "    for sup_module in model.modules():\n",
    "        for name, module in sup_module.named_children():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.p = 0.0\n",
    "            if isinstance(module, nn.Linear):\n",
    "                target_state_dict = module.state_dict()\n",
    "                bias = True if module.bias is not None else False\n",
    "                new_module = MixLinear(\n",
    "                    module.in_features, module.out_features, bias, target_state_dict[\"weight\"], 0.2\n",
    "                )\n",
    "                new_module.load_state_dict(target_state_dict)\n",
    "                setattr(sup_module, name, new_module)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be07db7a-90f6-49bb-9bdb-b1e1f45785d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(cfg, valid_loader, model, valid_df, fold, best_val_preds, best_val_score):\n",
    "    val_preds = []\n",
    "    val_losses = []\n",
    "    val_nums = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n",
    "            for (inputs, labels) in pbar:\n",
    "                inputs = collate(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(cfg.device)\n",
    "                labels = labels.to(cfg.device)\n",
    "                with autocast():\n",
    "                    loss, output = model(inputs, labels)\n",
    "                \n",
    "                output = output.detach().cpu().numpy()\n",
    "                val_preds.append(output)\n",
    "                val_losses.append(loss.item() * len(labels))\n",
    "                val_nums.append(len(labels))\n",
    "                pbar.set_postfix({\n",
    "                    'val_loss': loss.item()\n",
    "                })\n",
    "\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_loss = sum(val_losses) / sum(val_nums)\n",
    "    score = mcrmse(cfg, val_preds, valid_df)\n",
    "\n",
    "    val_log = {\n",
    "        'val_loss': val_loss,\n",
    "        'mcrmse': score\n",
    "    }\n",
    "    display(val_log)\n",
    "\n",
    "    if best_val_score > score:\n",
    "        print('\\033[31m'+'save model weight'+'\\033[0m')\n",
    "        best_val_preds = val_preds\n",
    "        best_val_score = score\n",
    "        torch.save(\n",
    "            model.state_dict(), \n",
    "            os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n",
    "        )\n",
    "    \n",
    "    return best_val_preds, best_val_score\n",
    "\n",
    "def training(cfg, train):\n",
    "    # =====================\n",
    "    # Training\n",
    "    # =====================\n",
    "    set_seed(cfg.seed)\n",
    "    oof_pred = np.zeros((len(train), 6), dtype=np.float32)\n",
    "    fold_score = []\n",
    "\n",
    "    for fold in cfg.trn_fold:\n",
    "        print(f'==========fold {fold}==========')\n",
    "        if (fold <= 7):\n",
    "            valid_df = train.loc[cfg.folds==fold]\n",
    "            valid_idx = list(valid_df.index)\n",
    "            best_val_preds = np.load(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'))\n",
    "            oof_pred[valid_idx] = best_val_preds.astype(np.float32)  \n",
    "        else:\n",
    "            # dataset, dataloader\n",
    "            train_df = train.loc[cfg.folds!=fold]\n",
    "            valid_df = train.loc[cfg.folds==fold]\n",
    "            train_idx = list(train_df.index)\n",
    "            valid_idx = list(valid_df.index)\n",
    "\n",
    "            # Datasetの設定\n",
    "            train_dataset = TrainDataset(cfg, train_df)\n",
    "            valid_dataset = TrainDataset(cfg, valid_df)\n",
    "            train_loader = DataLoader(\n",
    "                dataset=train_dataset, \n",
    "                batch_size=cfg.batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                drop_last=True,\n",
    "            )\n",
    "            valid_loader = DataLoader(\n",
    "                dataset=valid_dataset,\n",
    "                batch_size=cfg.batch_size,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            # model\n",
    "            model = CustomModel(cfg)\n",
    "            torch.save(model.config, cfg.EXP_MODEL+'config.pth')\n",
    "            model = reinit_bert(model)\n",
    "            # model = replace_mixout(model)\n",
    "            model = model.to(cfg.device)\n",
    "\n",
    "            # optimizer, scheduler\n",
    "            optimizer_grouped_parameters = get_optimizer_grouped_parameters(cfg, model)\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=cfg.decoder_lr, eps=cfg.eps, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            num_train_steps = int(len(train_df) / cfg.batch_size * cfg.n_epochs)\n",
    "            scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n",
    "\n",
    "            # enable FGM\n",
    "            fgm = FGM(model)\n",
    "\n",
    "            # model-training\n",
    "            best_val_preds = None\n",
    "            best_val_score = 9999\n",
    "\n",
    "            for epoch in range(cfg.n_epochs):\n",
    "                # training\n",
    "                print(f\"# ============ start epoch:{epoch} ============== #\")\n",
    "                train_losses = []\n",
    "                train_nums = []\n",
    "                model.train() \n",
    "                scaler = GradScaler(enabled=cfg.apex)\n",
    "                with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "                    for step, (inputs, labels) in enumerate(pbar):\n",
    "                        inputs = collate(inputs)\n",
    "                        for k, v in inputs.items():\n",
    "                            inputs[k] = v.to(cfg.device)\n",
    "                        labels = labels.to(cfg.device)\n",
    "                        with autocast(enabled=cfg.apex):\n",
    "                            loss, output = model(inputs, labels)\n",
    "\n",
    "                        pbar.set_postfix({\n",
    "                            'loss': loss.item(),\n",
    "                            'lr': scheduler.get_lr()[0]\n",
    "                        })\n",
    "                        train_losses.append(loss.item() * len(labels))\n",
    "                        train_nums.append(len(labels))\n",
    "\n",
    "                        if cfg.gradient_accumulation_steps > 1:\n",
    "                            loss = loss / cfg.gradient_accumulation_steps\n",
    "\n",
    "                        scaler.scale(loss).backward()\n",
    "\n",
    "                         # FGM attack\n",
    "                        fgm.attack()\n",
    "                        with autocast(enabled=cfg.apex):\n",
    "                            loss_adv, _ = model(inputs, labels)\n",
    "                        scaler.scale(loss_adv).backward()\n",
    "                        fgm.restore()\n",
    "\n",
    "                        if cfg.clip_grad_norm is not None:\n",
    "                            # scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(\n",
    "                                model.parameters(),\n",
    "                                cfg.clip_grad_norm\n",
    "                            )\n",
    "\n",
    "                        if (step+1) % cfg.gradient_accumulation_steps == 0:\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            optimizer.zero_grad()\n",
    "                            scheduler.step()\n",
    "\n",
    "                        if step % cfg.eval_step == 0 and step != 0:\n",
    "                            print(f'fold: {fold}, epoch: {epoch}, step: {step}')\n",
    "                            best_val_preds, best_val_score = evaluating(\n",
    "                                cfg, valid_loader,\n",
    "                                model,\n",
    "                                valid_df,\n",
    "                                fold,\n",
    "                                best_val_preds,\n",
    "                                best_val_score,\n",
    "                            )\n",
    "                            model.train()\n",
    "\n",
    "                train_loss = sum(train_losses)/sum(train_nums)\n",
    "                train_log = {\n",
    "                    'train_loss':train_loss\n",
    "                }\n",
    "                display(train_log)\n",
    "\n",
    "                # evaluating(epoch)\n",
    "                print(f'fold: {fold}, epoch: {epoch}, complete')\n",
    "                best_val_preds, best_val_score = evaluating(\n",
    "                    cfg, valid_loader,\n",
    "                    model,\n",
    "                    valid_df,\n",
    "                    fold,\n",
    "                    best_val_preds,\n",
    "                    best_val_score,\n",
    "                )\n",
    "\n",
    "            oof_pred[valid_idx] = best_val_preds.astype(np.float32)\n",
    "            np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'), best_val_preds)\n",
    "            fold_score.append(best_val_score)\n",
    "            del model; gc.collect()\n",
    "\n",
    "    np.save(os.path.join(cfg.EXP_PREDS, 'oof_pred.npy'), oof_pred)\n",
    "\n",
    "    # =====================\n",
    "    # scoring\n",
    "    # =====================\n",
    "    score = mcrmse(cfg, oof_pred, train)\n",
    "    print('fold score：', fold_score)\n",
    "    print('CV:', round(score, 4))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f311c7e6-6706-42a9-afea-46455ec9aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.16.2\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (21.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.16.2) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.16.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.16.2) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.16.2) (1.26.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.16.2) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.16.2) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895242 sha256=e82a3adf4635362cd97e424bb7fef02e64da25f6fd4ccc1db11051b3bebdf754\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "Successfully installed sacremoses-0.0.53 transformers-4.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tokenizers==0.11.6\n",
      "  Downloading tokenizers-0.11.6-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "Successfully installed tokenizers-0.11.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.9/dist-packages (4.16.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (1.23.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.1.96)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (3.19.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers[sentencepiece]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers[sentencepiece]) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers[sentencepiece]) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers[sentencepiece]) (1.26.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers[sentencepiece]) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers[sentencepiece]) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0menv: TOKENIZERS_PARALLELISM=true\n",
      "tokenizers.__version__: 0.11.6\n",
      "transformers.__version__: 4.16.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379a544dd3214fb7a0919e95672d20ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e672d7c1a5d4ee1b6e016b0b2810292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6fe185bbc042b18576874abd8c161f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cd2a76c5c54b8ea165f071430ee452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/668 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========fold 0==========\n",
      "==========fold 1==========\n",
      "==========fold 2==========\n",
      "==========fold 3==========\n",
      "==========fold 4==========\n",
      "==========fold 5==========\n",
      "==========fold 6==========\n",
      "==========fold 7==========\n",
      "==========fold 8==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7343b0a7f11f4087b36074f9ec0e414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:0 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241268671a1c49858e33edbf92aeb81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 0, step: 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae708d0baae4835937497082065192a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.3705314080733473, 'mcrmse': 1.9717572697542476}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 0, step: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998d2701f3e84ebd8723e0a5bfef0e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.2532525972637069, 'mcrmse': 0.725493926713183}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 0, step: 90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9fc13e599a4b9eb522fe11003e3320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.14489183853120755, 'mcrmse': 0.544104284522393}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 0, step: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b81c597cabc4943bae391898501106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11912651455310909, 'mcrmse': 0.4901843989925733}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11240698692515073, 'mcrmse': 0.4753789096361403}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 0, step: 210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8661b9f13b489da3eae061862c6a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.12153339401230483, 'mcrmse': 0.49416387591721056}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.45581109716810964}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 0, complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3d412df0e44a73a651fe836f0076b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11951347385221125, 'mcrmse': 0.48960237072462154}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:1 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ca7e72d1e44502871461b861b080a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, step: 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3e0fe32d4941a8b48ab19f0ad9a476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11915363525719289, 'mcrmse': 0.48960984310286326}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, step: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf9857a3fba430eb2e25a7f8249f62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11313326218548943, 'mcrmse': 0.4762638885824054}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, step: 90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aa1352b97643a6a00da0e6068825ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10799853215970652, 'mcrmse': 0.46553498150320466}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 1, step: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa4d2c0b12741c5ac38e8046a4d92b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11110127149411785, 'mcrmse': 0.47281555965453137}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, step: 150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983fcf6d590a49498063fe9535d82eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.11159386556319263, 'mcrmse': 0.4732513900903305}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, step: 180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a248465d571a4d0f938f957a1275857a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10825405822461828, 'mcrmse': 0.4663432229693645}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, step: 210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31d1636838f40c2afd3e0be417490e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10948890617207797, 'mcrmse': 0.46883641121799563}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.10893739837814462}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 1, complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e0a700c6684430b89a7727befbd9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10902454583998532, 'mcrmse': 0.46779172676534375}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:2 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d30350d472c48909643be097449040f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 2, step: 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53776853d50743f98f943345265aa74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10638136214688611, 'mcrmse': 0.461903531816246}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 2, step: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63739e1180d5422fb1dde8bfdff5628d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10601499709098236, 'mcrmse': 0.46139950010899433}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 2, step: 90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f72606530f42b4b78c8d2138d625da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10479191787864851, 'mcrmse': 0.45858377432215114}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 8, epoch: 2, step: 120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f738c9e55a5f4828bc941ab15d728108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10566706117004385, 'mcrmse': 0.460518088850884}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 8, epoch: 2, step: 150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f178cbb78eea4d9bb7d7fd9ba19b1900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10423037017245426, 'mcrmse': 0.45736640151293034}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 9, epoch: 4, step: 150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e150990771824b67af9bb88f1b6cb57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10853111196537407, 'mcrmse': 0.4659147654965008}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 9, epoch: 4, step: 180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5983836e3b904744be2f6a718aa2475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10839585001979556, 'mcrmse': 0.46559484190267436}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 9, epoch: 4, step: 210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481e5ee0e8e24256bfbec85f3d31a588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.10837838737940302, 'mcrmse': 0.4655514823888902}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.09330769083951707}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 9, epoch: 4, complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561a2f5feafc4e2f8046f916faf294d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.1083864235148138, 'mcrmse': 0.4655685214549422}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold score： [0.45382316722697025, 0.46486276854339237]\n",
      "CV: 0.4572\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Main\n",
    "# =====================\n",
    "\n",
    "# setup\n",
    "cfg = setup(Config)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, ElectraModel, ElectraTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import tokenizers\n",
    "import sentencepiece\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "\n",
    "# main\n",
    "train = pd.read_csv(os.path.join(cfg.INPUT, 'train.csv'))\n",
    "\n",
    "train = processing_features(train)\n",
    "\n",
    "cfg.tokenizer = ElectraTokenizer.from_pretrained(cfg.MODEL_PATH)\n",
    "cfg.tokenizer.save_pretrained(os.path.join(cfg.OUTPUT_EXP, 'tokenizer'))\n",
    "cfg.folds = get_multilabelstratifiedkfold(train, cfg.target_list, cfg.num_fold, cfg.seed)\n",
    "cfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'))\n",
    "score = training(cfg, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac87581e-7dec-4ab8-b624-31ef8da7cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.64.0)\n",
      "Requirement already satisfied: python-slugify in /usr/lib/python3/dist-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.8)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73031 sha256=db272934fb2cb5f222363996891b9f143efa47c3d1778a6e2f4610da61b2b492\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/b2/c3/fa4706d469b5879105991d1c8be9a3c2ef329ba9fe2ce5085e\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676809d5-a6a0-49cd-a7da-e7d9c44d6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload for file tokenizer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240k/240k [00:00<00:00, 315kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer.tar (240KB)\n",
      "Starting upload for file modelconfig.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.42k/2.42k [00:00<00:00, 5.10kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: modelconfig.pth (2KB)\n",
      "Starting upload for file model.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12.5G/12.5G [03:09<00:00, 70.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: model.tar (12GB)\n",
      "Starting upload for file fig.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.0k/10.0k [00:00<00:00, 18.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: fig.tar (10KB)\n",
      "Starting upload for file preds.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190k/190k [00:00<00:00, 288kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: preds.tar (190KB)\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844b3e7-9c0c-4c06-ab8a-65cd3edb6fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
