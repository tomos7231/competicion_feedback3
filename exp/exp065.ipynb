{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e87ca9a-aa97-4bf7-87a1-bc00cc707bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 26 15:53:17 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   43C    P8    17W / 230W |      0MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2053bec0-e260-4298-8646-e19b452a23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    AUTHOR = \"wanwan7123\"\n",
    "\n",
    "    NAME = \"feedback3-Exp065-deberta-v3-base\"\n",
    "    MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "    DATASET_PATH = []\n",
    "\n",
    "    COMPETITION = \"feedback-prize-english-learning\"\n",
    "\n",
    "    api_path = \"kaggle_json/kaggle.json\"\n",
    "\n",
    "    apex=True\n",
    "    seed = 42\n",
    "    num_fold = 10\n",
    "    trn_fold = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    batch_size = 16\n",
    "    n_epochs = 4\n",
    "    max_len = 768\n",
    "    target_list = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
    "    \n",
    "    weight_decay = 0.01\n",
    "    scheduler='cosine'\n",
    "    betas = (0.9, 0.999)\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    lr_weight_decay = 0.98\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    eval_step = 20\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps_rate=0.1\n",
    "    clip_grad_norm = 1000\n",
    "    gradient_accumulation_steps = 1\n",
    "    \n",
    "    awp_eps = 1e-2\n",
    "    awp_lr = 1e-5\n",
    "    awp_start = 1\n",
    "    \n",
    "    # GPU Optimize Settings\n",
    "    gpu_optimize_config= {\n",
    "        \"freezing\": False,\n",
    "        \"gradient_checkpoint\": True\n",
    "    }\n",
    "\n",
    "    upload_from_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb438435-24a6-4720-96e4-7fa73c0a4065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iterative-stratification\n",
      "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.23.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
      "Installing collected packages: iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.10.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp39-cp39-linux_x86_64.whl (1821.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.1+cu113) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0+cu116\n",
      "    Uninstalling torch-1.12.0+cu116:\n",
      "      Successfully uninstalled torch-1.12.0+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0+cu116 requires torch==1.12.0, but you have torch 1.10.1+cu113 which is incompatible.\n",
      "torchaudio 0.12.0+cu116 requires torch==1.12.0, but you have torch 1.10.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.10.1+cu113\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting text-unidecode\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: text-unidecode\n",
      "Successfully installed text-unidecode-1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import joblib\n",
    "import random\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ast import literal_eval\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, \n",
    "    KFold, \n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "! pip install iterative-stratification\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "! pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "!pip install text-unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83900b8-430a-440d-8ac6-d696e4ffb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(cfg):\n",
    "    cfg.COLAB = 'google.colab' in sys.modules\n",
    "    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # pip install\n",
    "    ! pip install transformers==4.16.2\n",
    "    ! pip install tokenizers==0.11.6\n",
    "    ! pip install transformers[sentencepiece]\n",
    "\n",
    "    # use kaggle api (need kaggle token)\n",
    "    f = open(cfg.api_path, 'r')\n",
    "    json_data = json.load(f) \n",
    "    os.environ['KAGGLE_USERNAME'] = json_data['username']\n",
    "    os.environ['KAGGLE_KEY'] = json_data['key']\n",
    "\n",
    "    # set dirs\n",
    "    cfg.INPUT = 'input'\n",
    "    cfg.EXP = cfg.NAME\n",
    "    cfg.OUTPUT_EXP = cfg.NAME\n",
    "    cfg.SUBMISSION = './'\n",
    "    cfg.DATASET = '../input/'\n",
    "\n",
    "    cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n",
    "    cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n",
    "    cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n",
    "\n",
    "    # make dirs\n",
    "    for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def dataset_create_new(dataset_name, upload_dir):\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n",
    "    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "    dataset_metadata['title'] = dataset_name\n",
    "    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fcca9f-a99e-4ca0-b230-8ad8e65ca9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Utils\n",
    "# =====================\n",
    "# Seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# KFold\n",
    "def get_kfold(train, n_splits, seed):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train)\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_stratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_groupkfold(train, target_col, group_col, n_splits):\n",
    "    kf = GroupKFold(n_splits=n_splits)\n",
    "    generator = kf.split(train, train[target_col], train[group_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_groupstratifiedkfold(train, target_col, group_col, n_splits, seed):\n",
    "    kf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col], train[group_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_multilabelstratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89beb2d-5c37-43d8-8b8b-1ee1ad722b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcrmse(cfg, preds, df):\n",
    "    all_score = 0\n",
    "    for i, column in enumerate(cfg.target_list):\n",
    "        score = np.sqrt(mean_squared_error(preds[:, i], df[column]))\n",
    "        all_score += score/len(cfg.target_list)\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63201c37-a808-46cf-938d-c412eeef851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章のバグを治す\n",
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c3a9b4a-c07c-4b54-8ef2-9264ec0387bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Dataset, Model\n",
    "# =====================\n",
    "\n",
    "def processing_features(df):\n",
    "    df['text'] = df['full_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "    return df\n",
    "\n",
    "# dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.text = df['text'].to_numpy()\n",
    "        self.labels = df[cfg.target_list].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.prepare_input(self.cfg, self.text[index])\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        return inputs, label\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_input(cfg, text):\n",
    "        inputs = cfg.tokenizer(text,\n",
    "                               add_special_tokens=True,\n",
    "                               max_length=cfg.max_len,\n",
    "                               padding=\"max_length\",\n",
    "                               truncation=True,\n",
    "                               return_offsets_mapping=False)\n",
    "        inputs['input_ids'] = torch.tensor(\n",
    "            inputs['input_ids'],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        inputs['attention_mask'] = torch.tensor(\n",
    "            inputs['attention_mask'],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6fc2e4e-b7b0-45d6-9068-87c9fbcc2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.gpu_optimize_config = cfg.gpu_optimize_config\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            cfg.MODEL_PATH,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout\": 0.,\n",
    "                \"hidden_dropout_prob\": 0.,\n",
    "                \"attention_dropout\": 0.,\n",
    "                \"attention_probs_dropout_prob\": 0,\n",
    "            }\n",
    "        )\n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "            cfg.MODEL_PATH,\n",
    "            config=self.config\n",
    "        )\n",
    "        self.fc = nn.Linear(self.config.hidden_size*4, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        self.ln = nn.LayerNorm(self.config.hidden_size)\n",
    "        self._init_weights(self.ln)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "\n",
    "        # Freeze\n",
    "        if self.gpu_optimize_config['freezing']:\n",
    "            freeze(self.backbone.encoder.layer[:8])\n",
    "\n",
    "        # Gradient Checkpointing\n",
    "        if self.gpu_optimize_config['gradient_checkpoint']:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        # batch, hidden_size\n",
    "        bert_outputs = self.backbone(**inputs)\n",
    "        # 最終4層をconcatenate\n",
    "        cls = torch.cat([bert_outputs[\"hidden_states\"][-1*i][:, 0] for i in range(1, 4+1)], dim=1)\n",
    "        logits1 = self.fc(self.dropout1(cls))\n",
    "        logits2 = self.fc(self.dropout2(cls))\n",
    "        logits3 = self.fc(self.dropout3(cls))\n",
    "        logits4 = self.fc(self.dropout4(cls))\n",
    "        logits5 = self.fc(self.dropout5(cls))\n",
    "        output = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.SmoothL1Loss(reduction='mean')\n",
    "            loss = loss_fct(output, labels)\n",
    "            return loss, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64a1030-4ae5-491f-a5e3-df06d10e9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_optimizer_grouped_parameters(cfg, model):\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "#         no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#         optimizer_parameters = [\n",
    "#             {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#              'lr': cfg.encoder_lr, 'weight_decay': cfg.weight_decay},\n",
    "#             {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#              'lr': cfg.encoder_lr, 'weight_decay': 0.0},\n",
    "#             {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n",
    "#              'lr': cfg.decoder_lr, 'weight_decay': 0.0}\n",
    "#         ]\n",
    "#         return optimizer_parameters\n",
    "    \n",
    "    \n",
    "def get_optimizer_grouped_parameters(cfg, model):\n",
    "    model_type = 'backbone'\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n",
    "             'lr': cfg.decoder_lr, 'weight_decay': 0.0},\n",
    "    ]\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n",
    "    layers.reverse()\n",
    "    lr = cfg.encoder_lr\n",
    "    for layer in layers:\n",
    "        lr *= cfg.lr_weight_decay\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": cfg.weight_decay,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "# initialize layer\n",
    "def reinit_bert(model):\n",
    "    for layer in model.backbone.encoder.layer[-1:]:\n",
    "        for module in layer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "    return model\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=int(num_train_steps*cfg.num_warmup_steps_rate), num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=int(num_train_steps*cfg.num_warmup_steps_rate), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726bbd52-23c9-4c89-9b1f-aec02e332c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        model: Module,\n",
    "        optimizer: Optimizer,\n",
    "        adv_param: str=\"weight\",\n",
    "        adv_lr: float=1.0,\n",
    "        adv_eps: float=0.01\n",
    "    ) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "    def attack_backward(self, inputs: dict, labels: Tensor) -> Tensor:\n",
    "        with autocast(enabled=self.cfg.apex):\n",
    "            self._save()\n",
    "            self._attack_step() # モデルを近傍の悪い方へ改変\n",
    "            adv_loss, _ = self.model(inputs, labels)\n",
    "            self.optimizer.zero_grad()\n",
    "        return adv_loss\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    # 直前に損失関数に通してパラメータの勾配を取得できるようにしておく必要あり\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(\n",
    "                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be07db7a-90f6-49bb-9bdb-b1e1f45785d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(cfg, valid_loader, model, valid_df, fold, best_val_preds, best_val_score):\n",
    "    val_preds = []\n",
    "    val_losses = []\n",
    "    val_nums = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n",
    "            for (inputs, labels) in pbar:\n",
    "                inputs = collate(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(cfg.device)\n",
    "                labels = labels.to(cfg.device)\n",
    "                with autocast():\n",
    "                    loss, output = model(inputs, labels)\n",
    "                \n",
    "                output = output.detach().cpu().numpy()\n",
    "                val_preds.append(output)\n",
    "                val_losses.append(loss.item() * len(labels))\n",
    "                val_nums.append(len(labels))\n",
    "                pbar.set_postfix({\n",
    "                    'val_loss': loss.item()\n",
    "                })\n",
    "\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_loss = sum(val_losses) / sum(val_nums)\n",
    "    score = mcrmse(cfg, val_preds, valid_df)\n",
    "\n",
    "    val_log = {\n",
    "        'val_loss': val_loss,\n",
    "        'mcrmse': score\n",
    "    }\n",
    "    display(val_log)\n",
    "\n",
    "    if best_val_score > score:\n",
    "        print('\\033[31m'+'save model weight'+'\\033[0m')\n",
    "        best_val_preds = val_preds\n",
    "        best_val_score = score\n",
    "        torch.save(\n",
    "            model.state_dict(), \n",
    "            os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n",
    "        )\n",
    "    \n",
    "    return best_val_preds, best_val_score\n",
    "\n",
    "def training(cfg, train):\n",
    "    # =====================\n",
    "    # Training\n",
    "    # =====================\n",
    "    set_seed(cfg.seed)\n",
    "    oof_pred = np.zeros((len(train), 6), dtype=np.float32)\n",
    "    fold_score = []\n",
    "\n",
    "    for fold in cfg.trn_fold:\n",
    "        print(f'==========fold {fold}==========')\n",
    "        if (fold <= 5):\n",
    "            valid_df = train.loc[cfg.folds==fold]\n",
    "            valid_idx = list(valid_df.index)\n",
    "            best_val_preds = np.load(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'))\n",
    "            oof_pred[valid_idx] = best_val_preds.astype(np.float32)  \n",
    "        else:\n",
    "            # dataset, dataloader\n",
    "            train_df = train.loc[cfg.folds!=fold]\n",
    "            valid_df = train.loc[cfg.folds==fold]\n",
    "            train_idx = list(train_df.index)\n",
    "            valid_idx = list(valid_df.index)\n",
    "\n",
    "            # Datasetの設定\n",
    "            train_dataset = TrainDataset(cfg, train_df)\n",
    "            valid_dataset = TrainDataset(cfg, valid_df)\n",
    "            train_loader = DataLoader(\n",
    "                dataset=train_dataset, \n",
    "                batch_size=cfg.batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                drop_last=True,\n",
    "            )\n",
    "            valid_loader = DataLoader(\n",
    "                dataset=valid_dataset,\n",
    "                batch_size=cfg.batch_size,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "            )\n",
    "\n",
    "            # model\n",
    "            model = CustomModel(cfg)\n",
    "            torch.save(model.config, cfg.EXP_MODEL+'config.pth')\n",
    "            model = reinit_bert(model)\n",
    "            # model = replace_mixout(model)\n",
    "            model = model.to(cfg.device)\n",
    "\n",
    "            # optimizer, scheduler\n",
    "            optimizer_grouped_parameters = get_optimizer_grouped_parameters(cfg, model)\n",
    "            optimizer = AdamW(optimizer_grouped_parameters, lr=cfg.decoder_lr, eps=cfg.eps, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
    "\n",
    "            num_train_steps = int(len(train_df) / cfg.batch_size * cfg.n_epochs)\n",
    "            scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n",
    "\n",
    "            # AWP\n",
    "            awp = AWP(\n",
    "                cfg,\n",
    "                model,\n",
    "                optimizer,\n",
    "                adv_lr=cfg.awp_lr,\n",
    "                adv_eps=cfg.awp_eps\n",
    "            )\n",
    "\n",
    "            # model-training\n",
    "            best_val_preds = None\n",
    "            best_val_score = 9999\n",
    "\n",
    "            for epoch in range(cfg.n_epochs):\n",
    "                # training\n",
    "                print(f\"# ============ start epoch:{epoch} ============== #\")\n",
    "                train_losses = []\n",
    "                train_nums = []\n",
    "                model.train() \n",
    "                scaler = GradScaler(enabled=cfg.apex)\n",
    "                with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "                    for step, (inputs, labels) in enumerate(pbar):\n",
    "                        inputs = collate(inputs)\n",
    "                        for k, v in inputs.items():\n",
    "                            inputs[k] = v.to(cfg.device)\n",
    "                        labels = labels.to(cfg.device)\n",
    "                        with autocast(enabled=cfg.apex):\n",
    "                            loss, output = model(inputs, labels)\n",
    "\n",
    "                        pbar.set_postfix({\n",
    "                            'loss': loss.item(),\n",
    "                            'lr': scheduler.get_lr()[0]\n",
    "                        })\n",
    "                        train_losses.append(loss.item() * len(labels))\n",
    "                        train_nums.append(len(labels))\n",
    "\n",
    "                        if cfg.gradient_accumulation_steps > 1:\n",
    "                            loss = loss / cfg.gradient_accumulation_steps\n",
    "\n",
    "                        scaler.scale(loss).backward()\n",
    "\n",
    "                        if cfg.clip_grad_norm is not None:\n",
    "                            # scaler.unscale_(optimizer)\n",
    "                            torch.nn.utils.clip_grad_norm_(\n",
    "                                model.parameters(),\n",
    "                                cfg.clip_grad_norm\n",
    "                            )\n",
    "\n",
    "                        # AWP\n",
    "                        if cfg.awp_start <= epoch:\n",
    "                            loss = awp.attack_backward(inputs, labels)\n",
    "                            scaler.scale(loss).backward()\n",
    "                            awp._restore()\n",
    "\n",
    "                        if (step+1) % cfg.gradient_accumulation_steps == 0:\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                            optimizer.zero_grad()\n",
    "                            scheduler.step()\n",
    "\n",
    "                        if step % cfg.eval_step == 0 and step != 0:\n",
    "                            print(f'fold: {fold}, epoch: {epoch}, step: {step}')\n",
    "                            best_val_preds, best_val_score = evaluating(\n",
    "                                cfg, valid_loader,\n",
    "                                model,\n",
    "                                valid_df,\n",
    "                                fold,\n",
    "                                best_val_preds,\n",
    "                                best_val_score,\n",
    "                            )\n",
    "                            model.train()\n",
    "\n",
    "                train_loss = sum(train_losses)/sum(train_nums)\n",
    "                train_log = {\n",
    "                    'train_loss':train_loss\n",
    "                }\n",
    "                display(train_log)\n",
    "\n",
    "                # evaluating(epoch)\n",
    "                print(f'fold: {fold}, epoch: {epoch}, complete')\n",
    "                best_val_preds, best_val_score = evaluating(\n",
    "                    cfg, valid_loader,\n",
    "                    model,\n",
    "                    valid_df,\n",
    "                    fold,\n",
    "                    best_val_preds,\n",
    "                    best_val_score,\n",
    "                )\n",
    "\n",
    "            oof_pred[valid_idx] = best_val_preds.astype(np.float32)\n",
    "            np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.npy'), best_val_preds)\n",
    "            fold_score.append(best_val_score)\n",
    "            del model; gc.collect()\n",
    "\n",
    "    np.save(os.path.join(cfg.EXP_PREDS, 'oof_pred.npy'), oof_pred)\n",
    "\n",
    "    # =====================\n",
    "    # scoring\n",
    "    # =====================\n",
    "    score = mcrmse(cfg, oof_pred, train)\n",
    "    print('fold score：', fold_score)\n",
    "    print('CV:', round(score, 4))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311c7e6-6706-42a9-afea-46455ec9aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.16.2\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (1.23.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (0.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.16.2) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.16.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.16.2) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.16.2) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.16.2) (2.8)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.16.2) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.16.2) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895242 sha256=5d678aef1a70b7bac8510ee0f5ae6cc0489f511cbfff26880f0765b0b21bdc2f\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "Successfully installed sacremoses-0.0.53 transformers-4.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tokenizers==0.11.6\n",
      "  Downloading tokenizers-0.11.6-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "Successfully installed tokenizers-0.11.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.9/dist-packages (4.16.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (5.4.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (2022.7.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (1.23.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (3.7.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (0.1.96)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from transformers[sentencepiece]) (3.19.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers[sentencepiece]) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers[sentencepiece]) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers[sentencepiece]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers[sentencepiece]) (2019.11.28)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers[sentencepiece]) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers[sentencepiece]) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0menv: TOKENIZERS_PARALLELISM=true\n",
      "tokenizers.__version__: 0.11.6\n",
      "transformers.__version__: 4.16.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78e59fd81f34db8bbd3b19c96a09783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e3c7f1609d4b0db03fa5782926e9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a101c44e7cf04b039a48057bbc29f478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========fold 0==========\n",
      "==========fold 1==========\n",
      "==========fold 2==========\n",
      "==========fold 3==========\n",
      "==========fold 4==========\n",
      "==========fold 5==========\n",
      "==========fold 6==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041b3633dc78475980e3ae2a63c9bd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:0 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b883543a2934502a233d044d4aa5058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 6, epoch: 0, step: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f944f0ce014af0854a8058e99ba67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.466019388354953, 'mcrmse': 3.035686632185041}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 6, epoch: 0, step: 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46e7107097a4a3080f204e31d742807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.1405455462463068, 'mcrmse': 1.7413429508759413}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Main\n",
    "# =====================\n",
    "\n",
    "# setup\n",
    "cfg = setup(Config)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, ElectraModel, ElectraTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import tokenizers\n",
    "import sentencepiece\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "\n",
    "# main\n",
    "train = pd.read_csv(os.path.join(cfg.INPUT, 'train.csv'))\n",
    "\n",
    "train = processing_features(train)\n",
    "\n",
    "cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_PATH)\n",
    "cfg.tokenizer.save_pretrained(os.path.join(cfg.OUTPUT_EXP, 'tokenizer'))\n",
    "cfg.folds = get_multilabelstratifiedkfold(train, cfg.target_list, cfg.num_fold, cfg.seed)\n",
    "cfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'))\n",
    "score = training(cfg, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87581e-7dec-4ab8-b624-31ef8da7cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676809d5-a6a0-49cd-a7da-e7d9c44d6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844b3e7-9c0c-4c06-ab8a-65cd3edb6fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
